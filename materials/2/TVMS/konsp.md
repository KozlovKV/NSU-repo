- [Инфо](#инфо)
- [23.09.04 - лекция](#230904---лекция)
  - [Основные понятия](#основные-понятия)
- [23.09.08 - семинар](#230908---семинар)
- [23.09.11 - лекция](#230911---лекция)
- [23.09.15 - семинар](#230915---семинар)
- [23.09.18 - лекция](#230918---лекция)
  - [Геометрическая вероятность](#геометрическая-вероятность)
  - [Колмогоровские аксиомы тервера](#колмогоровские-аксиомы-тервера)
  - [Условная вероятность](#условная-вероятность)
- [23.09.25 - лекция](#230925---лекция)
  - [Полная группа событий](#полная-группа-событий)
  - [Формула Байеса](#формула-байеса)
  - [Испытание Бернулли](#испытание-бернулли)
  - [Формула Пуассона](#формула-пуассона)
- [23.09.29 - семинар](#230929---семинар)
- [23.10.02 - лекция](#231002---лекция)
  - [Случайные величины](#случайные-величины)
  - [Биномиальное распределение](#биномиальное-распределение)
  - [Распределения Бернулли и Пуассона](#распределения-бернулли-и-пуассона)
  - [Поток событий](#поток-событий)
  - [Геометрическое распределение](#геометрическое-распределение)
- [23.10.06 - семинар](#231006---семинар)
  - [Условная вероятность](#условная-вероятность-1)
- [23.10.09 - лекция](#231009---лекция)
  - [Гипергеометрическое распределение](#гипергеометрическое-распределение)
  - [Абсолютно непрерывная функция распределения](#абсолютно-непрерывная-функция-распределения)
    - [Распределение Гаусса](#распределение-гаусса)
    - [Гамма-распределение](#гамма-распределение)
    - [Распределение Коши](#распределение-коши)
- [23.10.13 - семинар](#231013---семинар)
  - [Распределение Пуассона](#распределение-пуассона)
  - [Функция плотностного распределения](#функция-плотностного-распределения)


# Инфо
Препод - Рогозинский Сергей Валентинович

Семинарист - Насибулов Егор Андреевич

[Какой-то Классрум от 19-ГО, МАТЬ ЕГО, ГОДА](https://classroom.google.com/u/3/c/MTQ0Mzk0OTQyMjg5?pli=1)

# 23.09.04 - лекция
*Спустя 12 минут жоской преамбулы ни о чём, перешли к содержанию, которое тоже особо не нужно*

*Преамбула продолжается... Соответственно*

*Ещё 4 минуты прошло и... Вроде бы начали*

*Нет, не начали*

*Загадка от Жака-Фреско: какова вероятность начала лекции?* **$10^{-42}$**

*Для примера с автобусами не хватает [Вахтанга](https://www.youtube.com/shorts/9gy5TdhxlHU)*

*А у меня ведь правда есть таро...*

*Мораль кулстори: если контора требует от вас аспирантуры, вам не нужна такая контора... Тем временем позади 37 минут*

*Пожалуйста, не говори про используемые книги! Не надо!.. Это самое... Ну... Мы их читать не будем, соответственно. Минус половина пары*

*Столкнёмся с этими вещами?.. С ракетами что ли? А, супер, ладно - только теоретически. Хм... А оценивать шансы РФ в СВО будем?..*

*"Взорвалась при Рогозине, при Путине причём" - хмммм. На что он намекает???*

**Вроде как из книг лучшая у Юденко...**
![Книги](./books.JPG)

*А, лол. Тут в списке нет Юденко*


## Основные понятия
**Тервер** - раздел математики, изучающий математические модели случайных явлений, наблюдаемых при массовых повторениях испытаний

Тервер нужен для чёткой оценки вероятности событий и её сравнения

**Модель** - объект, отражающий важные для исследования свойства реального объекта

**Событие** - что-то, что происходит:
- Детерминированные - при определённых условиях мы получим ожидаемый результат
- Вероятностные - любой исход события не гарантирован

**Случайный эксперимент** - эксперимент, результат (исход) которого нельзя предсказать однозначно. Тервер изучает случайные эксперименты, удовлетворяющие условиям:
1. Эксперимент можно повторить в одинаковых условиях достаточно много раз
2. Исход A с увеличением количества повторений стремится к некоторому устойчивому числу относительно общего количества повторений эксперимента - то есть обладает **статистической устойчивостью**

**Случайность** - философское понятие. *Расходимся*

Изучение тервера бустили страховой бизнес, демография, статистика.

*Начинать военную спецоперацию против галлов **НЕ СТОИТ***

# 23.09.08 - семинар
Элементарный исход - взаимоисключающие простые исходы (то есть несовместные), которые при этом не могут быть разделены на более мелкие события. ОБъединение всех элементарных исходов даёт нам всё пространство исходов.

Вероятность события A - $P(A)$ - доля исходов, соответствующих событию A от общего числа исходов

Для независимых событий $P(AB) = P(A) * P(B)$.

$A|B$ - наступление события $A$ после события $B$
- $P(A|B) = \frac{P(AB)}{P(B)}$ - условная вероятность
- $P(B|A) = \frac{p(BA)}{P(A)}$ ($P(AB) = P(BA)$, чего нельзя сказать об $P(A|B)$ и $P(B|A)$)

Вероятность того, что произойдёт одно из событий $A,B$: $P(A + B) = P(A) + P(B) - P(AB)$

Элементарная комбинаторика:
- Перестановки $P_n = n!$
- Размещения $A_n^k = \frac{n!}{(n-k)!}$ - комбинация длиной k из множества длины n. Важен порядок
- Сочетания $C_n^k = \frac{a_n^k}{P_k} = \frac{n!}{k!(n-k)!}$

# 23.09.11 - лекция
Множество (пространство) элементарных событий обозначается $\Omega$

Говорят, что событие $A \subset \Omega$ произошло, если произошло хотя бы одно элементарное событие $\omega \in A$

**Классификация событий:**
- Достоверное событие - обязательно происходит при повторении опыта. $\Omega$ - достоверное
- Невозможное событие - никогда не происходит при повторении опыта. обозначается $\emptyset$
- Случайное событие

Событие $A$ **включено** в $B$: $\hArr \forall \omega \in A : \omega \in B$

Если $A \subset B \rArr A$ **влечёт** $B$

Событие $A, B$ **равны** $\hArr A \subset B$ и $B \subset A$

Умножение (пересечение), сложение (объединение) и вычитание событий происходит также, как в алгебре множеств и обозначается аналогично (*это, кстати, и для многих записей выше справедливо*)

**Несовместные события** - такие $A, B$, что $AB = \emptyset$

Вероятность определяется множеством разных методов:
- Субъективно
- Логически
- Частотно - частота появления данного исхода в длинной серии
- Классическое определение - соотношение благоприятных исходов к общему числу исходов
- Геометрическое - определение для континуального пространства элементарных событий
- Для общего случая 

# 23.09.15 - семинар
**Независимые события** - вероятность одного события не зависит от вероятности другого.

Вероятность происшествия двух независимых событий $P(AB) = P(A) * P(B)$

Но на самом деле определение множества независимых событий таково: множество событий $A = {a_1, ..., a_n}$ - независимое $\hArr \forall {a_{n1}, ..., a_{nk}} \subset A : P(a_{n1}, ..., a_{nk}) = P(a_{n1}) * ... * P(a_{nk})$

Если нужно выбрать из `n` `k` вариантов и `l` из `m`, то мы получаем формулу для комбинаций:
$$
\frac{C_n^k C_m^l}{C_{n+m}^{k+l}}
$$
**Важно!** Речь тут не о простом суммировании индексов. Фишка в том, что в знаменателе перестановки из общей мощности множество по мощности общей выборки. Это важно потому, что для некоторых подсчётов могут появиться дополнительные перестановки (например, количество способов выбора масти)

# 23.09.18 - лекция
## Геометрическая вероятность
**Геометрическая вероятность** заключается в бросании некой материальной точки в область $\Omega$. Вероятность попадания в область $A \subset \Omega$ обозначается $P(A) = \mu(A) / \mu(\Omega)$, где $\mu$ - мера области события. Условия организации пространства $\Omega$ позволяют утверждать, что выбор любой точки в нём **равновозможен**

## Колмогоровские аксиомы тервера
В качестве события должны рассматриваться только такие подмножества множества $\Omega$, операции над которыми приводят снова к событиям (*то есть область событий замкнута*)

Алгеброй $F$ называется **непустой класс подмножеств множества $\Omega$**, которые удовлетворяют условиям:
1. $\Omega \in F$
2. $A \in F \rArr \overline{A} \in F$
3. $A_1, ..., A_n \in F \rArr \cup_1^n A_i \in F$

Из этого определения мы можем вывести вероятностное пространство $<\Omega, S, P>$, где $\Omega$ - множество элементарных событий, $S$ - $\sigma$-алгебра, $P$ - функция, сопоставляющая каждому событию $A \in S$ число, называемое его вероятностью:
1. $0 \le P(A)$
2. $P(\Omega) = 1$
3. Для попарно непересекающихся $A_i, A_j$: $P(\cup_1_{\infty} A_i) = \sum_1^{\infty} P(A_i)$

## Условная вероятность
**О.** Вероятность события $A$ при условии, что произошло событие $B$: $P(A|B) = \frac{P(AB)}{P(B)}$

Универсальная формула для вероятности происшествия нескольких событий $P(AB) = P(A)P(B|A) = P(B)P(A|B)$

Таким же образом задаётся формула для произвольного количества событий: $P(A_1, A_2, A_3, ...) = P(A_1)P(A_2|A_1)P(A_3|A_1A_2)...$

**Независимые события** - события, вероятность которых никак не зависит друг от друга. Для них $P(A|B) = P(A)$ и наоборот. $\rArr P(AB) = P(A)P(B)$

# 23.09.25 - лекция
## Полная группа событий
**Полная группа событий** - группа попарно несовместных событий, то есть выполняются условия:
1. $\forall i,j : H_iH_j = \emptyset$
2. $\sum H_i = \Omega$

Если ${H_i}$ - полная группа и нам известны вероятности $P(H_i)$ и $P(A|H_i)$, то вероятность интересующего нас события $A$ будет $P(A) = \sum_{i=0}^n P(H_i)P(A|H_i)$

## Формула Байеса
Предположим у нас имеется полная группа ${H_i}$, каждое H в данном случае называется **гипотезой**, а $P(H_i)$ - его **априорная вероятность**. Если нам известные априорные вероятности и условные вероятности $P(A|H_i)$, то формула **апостериорной вероятности** $P(H_i|A) = \frac{P(H_i)P(A|H_i)}{P(A)}$. 

*Апостериорной обычно называется оценка пост-фактум. "Если произошло событие $A$, то какаова вероятность, что ему предшествовало именно событие $H_i$"* 

## Испытание Бернулли
**Схемой Бернулли** - эксперимент, удовлетворяющий условиям:
1. Эксперимент имеет 2 исхода ($A,\overline{A}$)
2. Эксперимент повторяется $n$ раз. При этом результат любого из экспериментов не должен влиять на любой другой
3. Вероятность двух исходов при каждом повторении исходного эксперимента одна и та же

Возьмём событие $p$ и $\overline{p} = q$

Будем считать за $P(n,m)$ - вероятность того, что при $n$ испытаниях некоторое событие произошло $m$ раз.

**Формула Бернулли:**
$$
P(n,m) = C_n^m p^m q^{n - m}
$$

*Пример:* частица пролетает мимо 6 счётчиков, каждый счётчик регистрирует частицу с вероятностью $0.8$. Частица считается обнаруженной (событие $A$), если она была замечена хотя бы двумя счётчиками. Найти вероятность события $A$.

- $p = 0.8$
- $q = 0.2$

Возьмём обратное событие $\overline{A} = A_0 + A_1$ - ни один датчик не сработал либо сработал только 1. Теперь применим формулу Бернулли 
$$
P(\overline{A}) = P(A_0) + P(A_1) = P(6, 0) + P(6, 1) = C_6^0q^6 + C_6^1pq^5 = (0.2)^6 + 6*0.8*(0.2)^5 = ... = 0.0016 \newline
\rArr P(A) = 1 - P(\overline{A}) = 0.9984
$$

<hr>

События в эксперименте Бернулли при разных $m$ **несовместны** $\rArr$ вероятность происшествия события при $n$ испытаниях от $m_1$ до $m_2$ раз = сумме их вероятностей для формулы Бернулли.

Формулу Бернулли очень больно использовать при большом количестве испытаний

## Формула Пуассона
Применяется в тех случаях, когда при **большом** количестве испытаний $n$ мала вероятность успеха:
$$
\lambda = np_n \newline
P(n, m) = C_n^m p^n q^{n-m} \rarr \frac{\lambda^m}{m!}e^{-\lambda}
$$
*Считается через формулу Бернулли и второй замечательный предел*

# 23.09.29 - семинар
**Геометрическая вероятность**

# 23.10.02 - лекция
## Случайные величины
**Случайная величина** - функция $X$, ставящая в соответствие каждому элементарному исходу $\omega \in \Omega$ число $x = X(\omega)$

[Определение функции распределения с семинара](#функция-плотностного-распределения)

Случайная величина $X$ называется дискретной, если существует конечное или счётное множество $\{x_k\}$ такое, что $\sum_{k=1}^{\infty}P(X = x_k) = 1$

*Дальше смотри там же, где и определение функции распределения*

## Биномиальное распределение
По сути, та же формула Бернулли, но немного с иной нотацией: также $n$ испытаний с вероятностью успеха $p$ и неуспеха $q = 1 - p$, но теперь мы обозначим количество успешных испытаний случайной величиной $X$ и получим формулу $P(X = k) = C_n^k p^k q^{n - k}$, также иногда пишут $X \equiv Bin(n, p)$. Через эту формулу можно построить таблицу/график распределения случайной величины $X$.

Не надо забывать об условиях биномиальных испытаний (испытаний Бернулли):
1. Только 2 исхода
2. Вероятность исходов неизменна
3. Все испытания независимы

## Распределения Бернулли и Пуассона
**Распределением Бернулли** называется испытание Бернулли при количестве испытаний = 1

**Распределением Пуассона** будто бы полностью копирует [формулу Пуассона](#формула-пуассона) с добавлением записи вида $X \equiv P(\lambda)$

## Поток событий
**Потоком событий** называется последовательность событий, которые наступают в случайные моменты времени.

**Интенсивность потока** $\lambda$ - среднее число событий за единицу времени. Если интенсивность потока - константа - и вероятность появления $k$ событий за время $t$ определяется формулой Пуассона $p_k = \frac{(\lambda t)^k}{k!}e^{-\lambda t}$, то поток событий называется **простейшим**

## Геометрическое распределение
Если проводятся независимые испытания, при который вероятность появления события $A$ - $p$, не появления - $q = 1 - p$. Если $X$ - число испытаний до первого события $A$, то вероятность, что событие произойдёт впервые на $k$-м испытании: $p_k = P(X = k) = pq^{k-1}$. Обозначается как $X \equiv Geom(p)$

# 23.10.06 - семинар
## Условная вероятность
Почти вся инфа [отсюда](#полная-группа-событий)

# 23.10.09 - лекция
## Гипергеометрическое распределение
Если из среди $N$ деталей есть $M$ стандартных, а мы хотим выбрать $n$ деталей так, чтобы среди них было $m$ стандартных (обозначим это событие за распределение $X$), то надо использовать несколько сочетаний:
- Всего вариантов выбрать $n$ изделий из $N$: $C_N^n$
- Число стандартных деталей $m$ из $M$ можно выбрать $C_M^m$ способами
- Тогда из $N - M$ нестандартных изделий требуемое количество нестандартных деталей $n - m$ можно выбрать $C_{N - M}^{n - m}$ способами

Получаем формулу $p_m = P(X = m) = \frac{C_M^m C_{N - M}^{n - m}}{C_N^n}$. Обозначается кратко такая формула как $X \equiv HG(N, M, n)$

Гипергеометрическое распределение моделирует число удачных выборок без возвращения из конечной совокупности (*что бы это ни значило... Наверное означает то, что совокупность деталей $N,M$ меняется при выборе*)

Если выборка относительно общего количества мала или выбор идёт из генеральной совокупности (*$N,M$ не меняются*), то можно использовать биномиальное распределение $X \equiv Bin(n, M/N)$

## Абсолютно непрерывная функция распределения
Если $F(x) = \int_{-\infty}^x f(u)du$, то функция распределения $F(x)$ называется **абсолютно непрерывной** (ещё её называют **непрерывной случайно величиной**), а $f(u)$ - **плотностью распределения** случайной величины $X$.

Если плотность распределения $f(x) = const, \forall x \in (a, b)$, то распределение на отрезке $(a, b)$ называют **равномерным**. Очевидно, что функция распределения в таком случае будет равномерно расти на $(a, b)$ (*то есть будет прямой*). Обозначается такая ситуация как $X \equiv U(a, b)$

**Экспоненциальное распределение** $X \equiv Exp(\lambda)$ - такое распределение, при котором плотность распределения $f(x) = \lambda e^{-\lambda x}, F(x) = 1 - e^{-\lambda x}, x > 0$, где $\lambda > 0$ - параметр распределения.

### Распределение Гаусса
**Нормальное (Гауссовское) распределение** $X \equiv N(a, \sigma)$ - плотность $f(x) = \frac{1}{\sigma \sqrt{2\pi}}^{-\frac{(x - a)^2}{2\sigma^2}}$, где $a, \sigma$ - параметры распределения. При $a = 0, \sigma = 1$ распределение называют **стандартным нормальным распределением**.

Интеграл Лапласса $Ф(z)$ позволяет считать вероятность $X \in (x_1, x_2)$ при нормальном распределении для заданных $a, \omega$: $z = \frac{x - a}{\omega}$. $Ф(-z) = -Ф(z)$. Интеграл Лапласа не берётся, поэтому используются численные методы либо таблицы значений.

**Правило трёх сигм**: для $X \equiv N(a, \sigma)$ вероятность отдаления значения $x$ от $a$ на расстояние больше $3\sigma$ близко к нулю.

Доказывается через простые вычисления (*если с табличной XD*):
$$
P(|X - a| < 3\sigma) = P(-3 < \frac{X - a}{\sigma} < 3) = Ф(3) - Ф(-3) = 2Ф(3) \approx 0.9973
$$
**Если правило трёх сигм для неизвестного закона распределения не выполняется, есть большие основания полагать, что распределения не является нормальным**

### Гамма-распределение
Плотность:
$$
f(x) = \frac{\lambda^a}{Г(a)}x^{a-1}e^{-\lambda x}, x > 0 \newline
a > 0, \lambda > 0, \newline
Г(a) = \int_0^{\infty}x^{a - 1}e^{-x}dx = (a - 1)!
$$
*Не понимаю, зачем давать такую сложную нотацию для Гамма-функции...*

Обозначается как $X \equiv Г(a, \lambda)$

### Распределение Коши
$$
X \equiv K \newline
f(x) = \frac{1}{\pi}\frac{1}{1 + x^2} \newline
F(X) = \frac{1}{2} + \frac{1}{\pi}\arctg x
$$

# 23.10.13 - семинар
## Распределение Пуассона
[Формула Пуассона](#формула-пуассона)

*Задача:*
Вероятность, что насекомое отложит k яиц = $P_{\lambda}(k) = \frac{\lambda^k}{k!}e^{-1}$

Вероятность вылупления яйца = P. Какова вероятность, что вылупится n яиц?

Яйца вылупляются независимо, поэтому при $k \ge n$ вероятность вылупления n яиц = $P^nP^{k-1}$, причём вариантов таких ситуаций может быть биноминальное количество, то есть $C_k^n$. Домножаем это на вероятность откладывания k яиц и получаем путём сокращений и суммирований (считая k от n до бесконечности), получаем формулу $\frac{(\lambda p)^n}{n!}e^{-\lambda p}$ 

## Функция плотностного распределения
**О.** $F(x) = P(\xi < x)$ - функция распределения случайной величины. (кси случайная, икс - заданное функцией число)

$$
F(-\infty) = 0 \newline
F(\infty) = 1 \newline
$$
$F(x)$ непрерывна слева и неубывает

$f(x) = F'(x)$ - плотность распределения

Определённый интеграл функции плотности распределения даст нам вероятность попадания числа в область интегрирования, то есть вероятность для отрезка $[a, b)$ будет равна $F(b) - F(a)$ (*причём включение или не включение граничных точек не играет роли, что следует из сути определённых интегралов*).

Для дискретного распределения мы не очень хотим считать плотность через производную. Чаще мы строим табличку с иксами и вероятностями выпадения именно этих значений.
