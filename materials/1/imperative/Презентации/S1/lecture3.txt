====================== Массивы ============================

Что такое массив. Термины:
  элементы/ячейки
  номера/индексы (с нуля)
  размер/длина

Объявление:
  int arr[200];   //объявление:  базовыйтип имяпеременной[длина];
  1) размер должен быть известен во время компиляции  (int main[n] --- нельзя)
  2) большие массивы лучше объявлять ВНЕ main (иначе программа может "упасть" сразу на старте)
Инициализация:
  int arr[10];                      //в arr может быть записан мусор
  int arr[5] = {1, 3, 2, 2, 5};     //массив с 5ю заданными значениями
  double arr[] = {1.0, 3.0, 2.3};   //компилятор определит размер по кол-ву элементов
  int arr[12] = {1, 2, 3, 5};       //остальные элементы зануляются
  int arr[2000] = {0};              //массив полностью зануляется

Индексация:
  arr[k] --- k-ый элемент массива arr
Примеры:
  int res = arr[3] + 7;             //читаем 3-ий элемент arr, прибавляем 7, записываем сумму в res
  for (int i = 0; i < n; i++)
    arr[i] = i;                     //записываем в i-ый элемент число i
  arr[i] += arr[i*i + i];           //можно писать выражения для индекса
  for (int i = 0; i < n; i++)
      arr[idx[i]] = i;              //индекс можно брать даже из другого массива

Выход за пределы массива (k < 0 || k > S-1) ---> может происходить что угодно!
Обычно происходит одно из:
  1) попадаем сильно мимо --- программа сразу падает (access violation)
  2) попадаем в мусор --- читаем оттуда что попало, запись ни на что не влияет, программа работает дальше
  3) попадаем в другую свою переменную --- читаем оттуда другие данные, запись изменяет другие переменные
  ...) язык C небезопасный =)
В VC при запуске в Debug запись за границы массива ИНОГДА отлавливается --- пишется ошибка типа "stack corruption".
На Linux можно использовать valgrind для поиска выхода за границу массива, также есть несколько sanitizer в Clang.

Простой пример (номер минимального элемента):
  int arr[1010];
  ...
  int main() {
    ...
    int n;
    scanf("%d", &n);
    for (int i = 0; i < n; i++)
      scanf("%d", &arr[i]);
    int bestValue = 1000000000;
    int bestIdx = 0;
    for (int i = 0; i < n; i++)
      if (bestValue > arr[i]) {
        bestValue = arr[i];
        bestIdx = i;
      }
    printf("%d", bestIdx);
  }

====================== Указатели ==========================

Память (виртуальная) ~=~ массив байтов  (~ 2^31-1 байтов всего, в 64-битном режиме больше)
ВСЕ переменные программы хранятся в нём.
Адрес байта = номер байта (135847584 = 0x01A5FC40 в hex)
Адрес переменной = номер первого её байта

Пример (на картинке):
  int x;   --- размер 4 байта --- занимает 4 байта подряд в памяти

"Указатель" = адрес (и тип)
  значение --- адрес чего-то
  дополнительно приписан тип для компилятора
Объявление:
  базовыйтип *имяпеременной;
  int *ptr;         //объявили переменную --- указатель на int
Взятие адреса/указателя (&амперсанд&):
  int x;
  ptr = &x;         //в ptr записываем адрес переменной x
                    //в этом случае говорят: ptr указывает на x
Разыменование указателя (*звёздочка*):
  int res = *ptr;   //читаем из памяти по адресу ptr значение типа int
Указатель на указатель? Легко:
  int **pp = &ptr;  //теперь в pp хранится адрес указателя ptr, а в ptr --- адрес переменной x =)

Арифметика на указателях: можно прибавить целое
  (ptr + k) --- указатель, который указывает на k-ый элемент после ptr
(картинка с байтами, где ptr+1, ptr+2, ptr+3, ptr-1, ptr-2 для целых чисел)
Работает как прибавление числа к адресу на ассемблере, но компилятор САМ умножает на размер элемента:
  адрес(ptr + k) = адрес(ptr) + k * размер(*ptr)

Индексация по указателю:
  ptr[k] эквивалентно *(ptr + k)  --- получить k-ое число, "отложив массив" от ptr
  ptr[0] эквивалентно *ptr
Переменная-массив в программе почти всегда автоматически превращается в указатель:
  int arr[128];  int *ptr = arr;    //в ptr запишется адрес массива arr
  arr[k] эквивалентно ptr[k]
  также это: *(arr+k) и *(ptr+k)
Байка про arr[k] и k[arr] =)

Арифметика на указателях: разность указателей (одного типа)
  int *ptrA, *ptrB;             //два указателя
  int k = ptrB - ptrA;          //разность = целое число
Число k должно получиться таким, что:
  ptrB == ptrA + k
Реально на ассемблере выполняется (asm):
  k = (адрес(ptrB) - адрес(ptrA)) / размер(*ptrA)
Разность адресов должна делиться на размер элемента (это правило нарушить очень сложно --- так что не паримся)

Нулевой указатель 0 --- указывает "ни на что":
  int *ptr = 0;
Читать/писать по нулевому указателю нельзя --- это ошибка.
Проверка на то, что указатель куда-то указывает:
  if (ptr) { ... }


===================== Асимптотика =========================

Цель: научиться оценивать время работы программы, а лучше даже алгоритма (когда программы ещё нет).
Допустим, на входе в алгоритм дан массив длины N.
Тогда нас интересует T(N) --- максимальное время работы программы на входе длины N (как функция от N/последовательность).
  входов длины N много, но мы выбираем максимум --- получаем время работы "в худшем случае"

Хочется оценивать скорость роста T(N):
  1) для больших N  (на маленьких и так всё летает, моргнуть не успеешь)
  2) с точностью до константы  (константа зависит от "аппаратуры", "компилятора", "деталей реализации" и пр.)
  3) оценивать будем сверху  (обычно нам важно понять, что на заданных объёмах программа укладывается в отведённое время)

Асимптотическая оценка сверху записывается так:
  T(n) = O(g(n))
гду g(n) --- какая-то положительная функция, показывающая скорость роста
Например:
  T(n) = O(n^2) означает, что на больших n время работы не превышает C * n^2  (для некоторой константы C).

O(...) --- это "O-большое".
Также известен как символ Ландау, возник и используется в математическом анализе.
Строгое определение:
  T(n) = O(g(n)), если:   существуют n0, C, такие что: для всех n > n0 верно: T(n) < C(g(n)) 
Альтернативные определения:
  T(n) / g(n) <= C при n > n0    (можно подобрать C и n0, чтобы это было верно)
Кроме того, можно определить через верхний предел:
  C0 = limsup(T(n) / g(n))
  тогда:
    если C0 конечное, то верно T(n) = O(g(n))    (причём можно брать любую константу C > C0)
    если C0 бесконечное, то T(n) = O(g(n)) неверно

Смотрим смысл определения на графике функции T(n)...

Замечания:

1) Оценки O-большое даются сверху, т.е. если T(n) = O(n^2), то и T(n) = O(n^3) тоже верно.
   Для точной оценки есть похожий символ Theta-большое.
   В программированим мы будем применять всегда O-большое, иногда говоря "точная" оценка или нет.

2) Кроме времени работы, также имеет смысл оценивать требуемую память M(n).

3) Время работы может сильно зависеть то только от количества элементов, но и от других параметров.
   Если такая зависимость есть, имеет смысл ввести доп. параметры и использовать их в асимптотической оценке.

4) Обычно функция g(n) положительна отделена от нуля, поэтому в определении можно полагать n0 = 0.
   Правда тогда константа C будет довольно бессмысленной: как программистам, нам обычно интересна константа на больших объёмах.

Примеры:
  сумма чисел в массиве длины N   --- T(N) = O(N)
  проверка числа N на простоту    --- T(N) = O(sqrt(N))      (если перебирать делители до sqrt(M))
  for i=[0..n) for j=[i+1..n) ... --- T(N) = O(N^2)


Зачем нужно оценивать асимптотику?
Обычно вы оцениваем количество операций, а не время --- в принципе, они пропроциональны друг другу.
Сколько простых операций в секунду делает компьютер:
Тактовая частота ~= 2-4 GHz  ==>  10^9 тактов в секунду  =>  10^8 операций в секунду (примерно)

Для какого характерного размера решение с известной асимптотикой будет работать меньше секунды:
  T(N) = O(1), O(log(N))    N <= ???   (всё пройдёт, рост сильно медленный)
  T(N) = O(sqrt(N))         N <= 10^15
  T(N) = O(N)               N <= 10^7   (или 10^8)
  T(N) = O(N log(N))        N <= 10^6
  T(N) = O(N sqrt(N))       N <= 10^5
  T(N) = O(N^2)             N <= 10^4
  T(N) = O(N^3)             N <= 500
  T(N) = O(N^4)             N <= 100
  T(N) = O(2^N)             N <= 26
  T(N) = O(N!)              N <= 13
Рекомендация: по ограничениям на размер данных в условии задачи смотреть, какой асимптотики решение уложится по времени.
Лучше всего писать самое простое решение, которое укладывается по времени --- не обязательно писать самое быстрое.
