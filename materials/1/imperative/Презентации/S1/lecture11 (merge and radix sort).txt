======================== Quicksort =========================

Главная проблема быстрой сортировки:  размеры половин могут быть разными.
Пусть в левую попало L элементов, а в правую --- R = K - L.
Тогда:
  1) Если L = 0 или R = 0, то скорее всего сортировка зависнет, уходя в бесконечную рекурсию.
  2) Если L/K сильно сильно близко к 0 или 1, то сортировка будет работать медленно.

Можно нарисовать дерево вызовов quicksort.
Время работы всей сортировки пропорционально суммарной длине всех отрезков.
На каждом уровне суммарная длина не больше N.
Значит если глубина H, тогда время работы будет не больше O(N H).

Например, если всегда делить так, что в меньшую половину попадает хотя бы 10% элементов, тогда:
  H = log(N) / log(10/9) ~= 6.5 * log2(N) = O(log N)
То есть даже в таком случае время работы O(N log N).

Однако, если всегда L = 1, то время работы будет точно O(N^2).


Соотношение размеров половин зависит от стратегии выбора пивота:
  1) Первый или последний элемент берётся как пивот.
     Тогда на уже отсортированном массиве будет работать O(N^2) времени --- сильно медленно.
  2) Средний элемент берётся как пивот.
     Будет работать квадратичное время на специально сгенерированном antiqsort-тесте.
     При любой детерминированной стратегии выбора пивота можно сгенерировать такой тест.
  3) Выбираем в качестве пивота случайный элемент массива.
     Получается время работы O(N log N) в среднем на любом массиве без одинаковых элементов (док-во будет далее).
     В худшем случае время работы O(N^2), но вероятность такого поведения крайне мала.

Генератор псевдослучайных чисел:
  #include <stdlib.h>
  int idx = rand();       //возвращает случайное число от 0 до RAND_MAX  (на MSVC равно 32767)
Для увеличения диапазона на VC можно делать ((rand() << 15) + rand()).
Для сокращения диапазона можно делать % n.


Что делать, когда много одинаковых элементов?
Если значения, равные пивоту, кидать всегда в одну сторону, тогда будет зависание или O(N^2) время работы.
Хорошие решения проблемы:
  1) Кидать элементы, равные пивоту, по очереди влево и вправо.
  2) Если элемент равен пивоту, то выбирать его половину случайным образом.
Тогда даже если много элементов, равных пивоту, они будут раскидываться примерно поровну.
Важный момент: нужно гарантировать, что с каждой стороны есть хотя бы один элемент!

Выводы:
  1) время работы O(N^2) в худшем случае
  2) пивот лучше выбирать случайным образом
     равные элементы надо раскидывать примерно поровну
     тогда: время работы O(N log N) в среднем

>>> Пусть все элементы различные, а пивот выбирается случайным образом.
Докажем, что среднее время работы равно O(N log N).
Обозначим среднее время на массиве размера N через T(N).

Предположение индукции:
  T(N) <= a N log2 N + b N           (для некоторых констант a, b > 0)
База:
  T(1) = q0    <= b * 1    (при b > q0)

Шаг индукции?
  Заметим, что пивот выбирается N способами, все способы равновероятны.
  Если пивот k-ый по величине, то в левую половину попадает k элементов, а в правую --- (N-k) элементов.
  Исключение: когда пивот максимальный, алгоритм должен запихнуть пивот влево --- тогда будет деление N-1 : 1.

  T(N) = 1/N * (
    T(1) + T(N-1) +
    T(2) + T(N-2) +
    T(3) + T(N-3) +
    ...
    T(k) + T(N-k) +         //столько времени нужно на два рекурсивных запуска
    ...                     //если пивот k-ый по величине
    T(N-2) + T(2) +
    T(N-1) + T(1) +
    T(N-1) + T(1)           //(специальный случай)
  ) + q1 N                  //partition занимает линейное время от размера массива

  T(k) + T(N-k) = a k log2 k + a (N-k) log2 (N-k) + b k + b (N-k)
  Можно оценить:
    log2 k, log2 (N-k) <= log2 N
  Тогда:
    T(k) + T(N-k) <= a k log2 N + a (N-k) log2 N + b k + b (N-k) = a N log2 N + b N
  Если поставить это в оценку T(N), получим:
    T(N) <= 1/N * (N * (a N log2 N + b N)) + q1 N = a N log2 N + b N + q1 N
  Это никак не может быть меньше  a N log2 N + b N  из-за добавки:     q1 N   => не получится так доказать!
  По идее, нужно оценить сумму поточнее, чтобы оценка была меньше на линейную составляющую от N, которая сможет поглотить q1 N.
  
  Заметим, что когда partition делит массив примерно поровну, можно получить более точную оценку логарифма.
  Если N/4 <= k <= 3N/4, тогда:
    log2 k, log2 (N-k) <= log2(3/4 N) = log2 N + log2(3/4) = log2 N - log2(4/3)
                                                                       ^^^ 
                                                                       обозначим эту штуку через c > 0
  Тогда подставив такую оценку, получим:
    T(k) + T(N-k) <= a N (log2 N - c) + b N     при k in [N/4, 3 N/4]
                  <= a N log2 N + b N           иначе
  Заметим, что в половине случаев оценка стала меньше на линейную составляющую:  a c N
  Поставим это в сумму для T(N), и получим:
    T(N) <= a N (log2 N - c/2) + b N + q1 N = a N log2 N + b N + (q1 - a c/2) N <= A N log2 N + b N
                                                                   ^^^^^^^
                                                                 отрицательно при a > (2 q1 / c)

Получается, что можно выбрать подходящие значения констант a и b.


================== Сортировка сравнениями ==================

Вопрос: можно ли отсортировать быстрее, чем за O(N log N) ?

Представим себе, что каждый элемент массива --- это чёрный ящик, который можно только переставлять с места на место.
Конечно, на таких условиях отсортировать массив нельзя, нужно как-то извлекать информацию об элементах.
Дополнительно разрешим алгоритму сравнивать элементы: он может сравнить два элемента и узнать, какой из них меньше.
Сортировки, которые работают на таких условиях, называются "сортировки сравнениями".
Все пройденные до этого момента сортировки (insertion sort, mergesort, quicksort) являются сортировками сравнениями.

Пусть имеется сортировка сравнениями, которая:
1) всегда правильно работает
2) детерминированная (нет случайности)
Докажем, что в этом случае она работает как минимум за O(N log N) в худшем случае.

Заметим, что любую сортировку сравнениями можно переделать следующим образом.
Можно считать, что элементы массива не переставляются, они лишь сравниваются.
В конце алгоритмы выдаёт перестановку: как нужно переставить элементы, чтобы они стали правильно упорядочены.

Пример:
  1) abb               7) aab
  2) ba       ---->    4) aabba
  3) bba               5) abab        сортировка возвращает порядок:
  4) aabba    ---->    1) abb             7 4 5 1 6 2 3
  5) abab              6) b           
  6) b        ---->    2) ba
  7) aab               3) bba

Нестрогое доказательство:
  Всего перестановок N!  (факториал).
  За каждый вопрос типа ?Ai < Aj? алгоритм узнаёт 1 бит информации о порядке (ответ или "да", или "нет").
  Всего нужно узнать информации: log2(N!) бит.
  Значит сравнений должно быть хотя бы столько.

  Есть формула Стирлинга:
    N! ~= (N/e)^N sqrt(2 pi N)     (для больших N)
  Используя её, можно оценить:
    log2(N!) >= log2((N/e)^N) = N (log2 N - log2 e) = N log2 N - O(N)
  Это означает, что меньше O(N log N) по порядку работать никак не может.

Строгое доказательство:
  Для любой такой сортировки можно нарисовать дерево сравнений.
  Например, чтобы отсортировать три элемента, можно использовать такое дерево:

                       A0 < A1?
                   (да)        (нет)
           A1 < A2?                 A2 < A1?
         (да)   (нет)             (да)      (нет)
    [0 1 2]       A0 < A2?     [2 1 0]          A0 < A2?
                (да)   (нет)                 (да)     (нет)
            [0 2 1]     [2 0 1]          [1 0 2]        [1 2 0]
                  
  Дерево полностью показывает, когда какие вопросы задавать, и какой порядок вывести в качестве ответа в каком случае.
  Максимальное количество вопросов равно высоте дерева H.
  Дерево бинарное (после вопроса всегда два варианта), это значит, что на каждом следующем уровне максимум в два раза больше вершин.
  Значит у дерева высоты H не может быть больше 2^H листьев.

  В листьях записаны ответы-перестановки. Они могут совпадать между собой.
  Однако если сортировка правильная, то каждая возможная перестановка встречается хотя бы в одном листе.
  Если это не так, то можно легко подогнать входной массив с разл. элементами, на котором правильным ответом будет отсутствующая перестановка, и алгоритм выведет что-то другое.
  Значит листьев должно быть как минимум N!

  Получаем: 2^H >= N!   =>  H >= log2(N!) = N log2 N - O(N)
  То есть такая сортировка должна делать H сравнений в худшем случае, а значит работать хотя бы O(N log N).


==================== Сортировка подсчётом ==================
(counting sort)

Рассмотрим пример сортировки, которая НЕ является сортировкой сравнениями.
Пусть известно, что ключи элементов целые, лежат в диапазоне от 0 до C-1.
(обычно элементы состоят из "ключа", по которому надо сортировать, и "значение", которое прикреплено к ключу и никакого влияния на процесс не оказывает)

Заведём C списков.
В каждый список L[x] будем записывать все элементы с ключом, равным x.
Тогда в конце останется только составить эти списки один за другим в порядке увеличения x.

Алгоритм:
  for i in [0..N)
    x = A[i].key
    add(L[x], A[i])
  Ans = []
  for x in [0..C)
    for e in L[x]:
      add(Ans, e)

Каждый элемент просматривается ровно два раза, однако глупо будет говорить, что алгоритм работает за O(N).
В данном случае также очень важно значение C.
Алгоритм требует O(N + C) времени и памяти.

На практике этот алгоритм нужно реализовывать по-умному, чтобы работало быстро.
Не надо заводить кучу списков L[*], нужно сразу определить, куда в Ans попадают элементы с ключом x, и затем аккуратно их раскидать.

В первую очередь, нужно почитать гистограмму ключей, т.е. сколько раз встречается каждый ключ.
Это делается одним проходом по массиву A.
Далее вычисляем массив starts, где starts[x] показывает позицию, с котором начинаются элементы с ключом x в выходном массиве Ans.
Это вычисляется одним проходом по гистограмме (префиксные суммы).
Наконец, нужно раскидать все элементы на свои места за второй проход по A
В ходе прохода надо для каждого x хранить и поддерживать позицию, в которую надо будет поставить следующий элемент с ключом x.
Изначально эта позиция равна starts[x], в процессе раскидывания она увеличивается до starts[x+1].


================== Цифровая сортировка =====================
Также называется "radix sort".

Будем говорить, что сортировка устойчивая (stable), если элементы с одинаковым ключом в выходном массиве идут в том же порядке, что и во входном массиве.
(это имеет значение, если у элементов помимо ключей есть приписанные значения)

Примеры:
  quicksort --- не устойчивая
  mergesort --- при аккуратной реализации можно сделать устойчивой
  counting sort --- устойчивая

На самом деле, можно легко сделать любую сортировку сравнениями устойчивой.
Для этого надо в каждому элементу прикрепить его индекс во входном массиве.
Далее когда надо сравнить два элемента, мы сравниваем их как обычно по ключам.
Если ключи совпадают, тогда смотрим на индекс во входном массиве и сравниваем по нему.
В конце сортировки индексы стираем.
Таким образом, приписанный индекс выступает как "tie breaker", и с точки зрения сортировки все элементы уже получаются различными.


Radix sort работает над векторами фиксированной длины, каждая координата вектора считается "цифрой".
При желании можно применять её к строкам произвольной длины, к целым числам, к вещественным числам.
Векторы упорядочиваются лексифографически: сначала по старшей цифре, при равенстве упорядочиваются по цифре перед ней, и т.п.
Мы рассмотрим применительно к целым числам.

Пусть ключи являются целыми числами uint32_t (в диапазоне от 0 до 2^32-1).
Каждый ключ можно разбить на 4 байта:

       +------+------+------+------+
   X = | X<3> | X<2> | X<1> | X<0> |
       +------+------+------+------+
       старший               младший

Каждый байт --- это цифра от 0 до 255.

Тогда цифровая сортировка работает так:
  for b = [0..3]:
    CountingSort(A, by b-th byte)       //сортируем A, используя b-ый байт как ключ

ВАЖНО: сначала сортируем по младшему разряду, а в конце --- по старшему!

Докажем, что это правильно работает.
Индукционное предположение:
  В начале b-ой итерации массив отсортирован по младшим b байтам  (т.е. как будто все байты старше просто стёрли)
База:
  При b=0 любой порядок подходит под утверждение (все цифры "стёрли")
Переход:
  Заметим, что при сортировке по b цифрам надо прежде всего упорядочить по старшей, (b-1)-ой цифре.

    ???|1|10123 \
    ???|1|24132  | блок элементов с 1 в старшем разряде
    ???|1|24234  |
    ???|1|40123 /
    ???|3|04143 >- блок элементов с 3 в старшем разряде
    ???|4|11245 \
    ???|4|14234  | блок элементов с 4 в старшем разряде
    ???|4|31231 /
    ???|5|31243 \_ блок элементов с 5 в старшем разряде
    ???|5|40410 /
     ^  ^  ^^^ 
     |  |  \:: хвост из (b-1) младших цифр
     |  |
     |  +----- (b-1)-ая цифра (по ней мы только что отсортировали)
     +-------- старшие цифры --- пока они значения не имеют

  После последней сортировки элементы точно отсортированы по старшему ((b-1)-ому) разряду.
  Остаётся вопрос, правильный ли порядок внутри элементов одного блока (они с точки зрения последней сортировки "равны").
  Заметим, что до последней сортировки массив был отсортировам по хвосту (предположение индукции).
  Сортировка подсчётом устойчивая, поэтому внутри блока порядок сохраняется: были отсортированы по хвосту, значит и теперь отсортированы по хвосту.
  Получается, что порядок действительно верный.

Время работы: O(K (N + B)), где K --- количество цифр, и каждая цифра в диапазоне [0..B).

Можно заметить, что если ключи лежат в диапазоне [0..C), то нужно не более logB(C) = log C / log B цифр.
Тогда время работы можно записать так: O((N+B) * log(C)/log(B)).

Например, для сортировки 32-битных целых по байтам получаем O(4 * (N + 256)).
Нужно всего лишь 8 проходов по массиву, чтобы отсортировать его.