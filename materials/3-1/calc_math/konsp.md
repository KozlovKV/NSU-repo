# Инфо
Лектор - Васкевич <3

Семинарист - Левыкин Александр Иванович

# 24.09.03 - Лекция
Классификация вычислительных алгоритмов:
- Матричное вычисление
- Задачи оптимизации, решение нелинейных уравнений
- Интерполирование и численное дифференцирование
- Численное приближение не-полиномов
- Численное интегрирование
- Численные и разностные методы для обыкновенных диффуров

## Корень числа
Простейший численный метод - нахождение положительного квадратного корня числа $a$. Испульзуем рекуррентную формулу:

$$
x_n = \frac{1}{2}(x_{n-1} + \frac{a}{x_{n-1}})
$$

Доказывается через выведение погрешности эпсилон, которая будет убывать быстрее геометрической прогрессии с шагом $1/2$, благодаря чему $\lim_{n \rarr \infty} x_n = \sqrt{a}$

*И... Реально работает! Вот код на Питоне:*
```python
a = 2
x = 1.0
for _ in range(1000):
    x = (x + a / x) / 2.0
print(x)
```

## Общие правила численного метода
1. Исходная непрерывная задача замещается дискретной задачей (см. пример выше, где мы избавились от непрерывной функции корня)
2. Добавляется количество шагов (параметр $N$, также называется **дискретным временем**)
3. Увеличивая дискретное время, мы приблизим результат численного метода к результату непрерывного сколь угодно близко (*на третий принцип порой забивают и даже очень часто - в зависимости от решаемой задачи*)

## Floating numbers
Описывается множеством $p, t, L, U$

Нормализованные числа с плавающей точкой обозначаются как множество $F_1$, оно обладает следующими свойствами:
- Числа в нём распределены неравномерно
- В частности, чем больше модуль числа, тем больше расстояние до соседей

По стандарту IEEE 754 тип данных float имеет такое распределение:
- 1 бит - знак
- 8 бит - экспонента
- 23 бита - мантисса

*На double мы тут, видимо, забили... А, нет, слегка упомянули. ну в общем, это всё мы итак знаем*

Очень часто числа округляются. Есть несколько способов:
- Вниз - $R_d(x)$ - отбрасываем нужную часть мантиссы
- Вверх - $R_u(x)$ - стандартные правила округления
- Чётное - $R_e(x)$ - отличается от округления вверх только в случае, если $x$ находится ровно между двумя ЧПТ, тогда из них берётся то число, мантисса которого заканчивается на чётное число

*А дальше чё-то было про абсолютную и относительную погрешность, что, наверное, важно, но мне показалось невероятно скучным*

# 24.09.04 - Семинар
Абсолютная погрешность: $\Delta(a^*) = ||a^* - a||$

Относительная погрешность: $\delta(a^*) = \frac{||a^* - a||}{||a^*||}$

## Векторные и матричные нормы
$$
||\vec{X}||_1 = \sum |x_i| \\
||\vec{X}||_2 = \sqrt{\sum x_i^2}
||\vec{X}||_{\infty} = \max_i |x_i|
$$

Нормы $||.||_a, ||.||_b$ эквивалентны, если
$$
\forall X : \exist C_1, C_2 : C_1||X||_b \le ||X||_a \le C_2||X||_b
$$

# 24.09.10 - Лекция
## Численное решение СЛАУ
Разумеется, СЛАУ можно решить, в случае невырожденной матрицы (а иначе она и не решится) методом Крамера, однако считать определитель матрицы - задача не самая лёгкая

При этом в целом на компе при $n \le 10^6$ определитель считается за разумное время, однако есть и более эффективные методы: прямой и итерационный

Однако для этих численных методов возникает погрешность. Особенно высока она будет в плохо обусловленных системах. Таким образом становится актуальным оценка обусловленности матрицы

## Нормы (ещё раз)
Все нормы линейны и удовлетворяют неравенству треугольника

Стандартные нормы:
- $||\vec{u}||_{\infty} = \max_{1 \le i \le n} |u_i|$ - кубическая
- $||\vec{u}||_1 = \sum_{i=1}^n |u_i|$ - октаэдрическая
- $||\vec{u}||_2 = \sqrt{\sum_{i=1}^n |u_i|^2}$ - евклидова

Пространство $\R^n$ снабжённое одной из норм будем обозначать как $L^n$

Теперь определим норму для матрицы $A$:
$$
\forall ||.|| : ||A|| = \sup_{\vec{u} \ne 0} \frac{||A\vec{u}||}{||\vec{u}||}
$$
Эта норма также удовлетворяет:
- неравенству треугольника
- однородна ($||\lambda A|| = |\lambda|||A||$)
- неравенству треугольника 2.0 ($||AB|| \le ||A||||B||$)

Таким образом, из стандартных норм векторов получаем такие нормы для матриц:
- $||A||_{\infty} = \max_{1 \le i \le n} \sum_{j=1}^n |a_{ij}|$ - ищем максимальную по сумме строку матрицы
- $||A||_1 = \max_{1 \le j \le n} \sum_{i=1}^n |a_{ij}|$ - почти то же, что и первое, но теперь ищем максимальную сумму по столбцам
- $||A||_2 = \sqrt{\max_{1 \le i \le n} \lambda_i(A^*A)}$
  - $\lambda_i(A^*A)$ - это с.з. матрицы $A^*A$, при этом $A^* = A^T$
  - Если $A$ симметричная, тогда $\lambda_i(A^*A) = \lambda_i(A^2) = |\lambda_i(A)|^2$

*Напоминалка:* скалярное произведение $(\vec{u}, \vec{v}) = \sum_{i=1}^n u_iv_i$

Из определения нормы матрицы следует, что если норма матрицы согласована с нормой векторов, то для единичной матрицы норма будет 1

## Обусловленность матрицы
**О.** $\mu(A) = ||A||||A^{-1}||$ называется **числом обусловленности** матрицы. Несложно заметить, что выбор нормы влияет на числа обусловленности. Также обозначается как $cond(A)$

**Т.** Если возмущение матрицы $\Delta A$ таково, что:
$$
\mu(A) \frac{||\Delta A||}{||A||} < 1
$$
Тогда возмущение вектора-решения $\Delta \vec{u}$ удовлетворяет оценке:
$$
\frac{\Delta \vec{u}}{\vec{u}} \le \frac{\mu(A)}{1 - \mu(A) \frac{||\Delta A||}{||A||}}(\frac{||\Delta \vec{f}||}{||\vec{f}||} + \frac{||\Delta A||}{||A||})
$$
*Доказывается через преобразования последней оценки при помощи $A\vec{u} = \vec{f}$ и неравенства треугольника*

**С. 1** При $\Delta A \approx 0$, получаем оценку погрешности только правой части:
$$
\frac{\Delta \vec{u}}{\vec{u}} \le \mu(A)\frac{||\Delta \vec{f}||}{||\vec{f}||}
$$

**С. 2** *Тут какая-то ещё непонятная оценка*

В силу неравенства треугольника 2.0 и $||AA^{-1}|| = ||E|| = 1$ получаем, что $\mu(A) \ge 1$

Если число обусловленности не превышает 10, мы говорим о слабом влиянии ошибок входных данных. При $\mu >> 100$ система плохо обусловлена

*Дальше идёт пример с матрицей $2*2$, который бы хорошо осознать, но я не осознал*

## Прямые методы решения СЛАУ
...

# 24.09.11 - Семинар
*1.5 часа доказывали формулы с лекции...*