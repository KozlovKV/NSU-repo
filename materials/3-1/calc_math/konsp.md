- [Инфо](#инфо)
- [Лекция 1](#лекция-1)
  - [Корень числа](#корень-числа)
  - [Общие правила численного метода](#общие-правила-численного-метода)
  - [Floating numbers](#floating-numbers)
  - [Погрешности](#погрешности)
  - [Неустойчивость](#неустойчивость)
- [Семинар 1](#семинар-1)
  - [Векторные и матричные нормы](#векторные-и-матричные-нормы)
- [Лекция 2](#лекция-2)
  - [Численное решение СЛАУ](#численное-решение-слау)
  - [Оценки погрешности](#оценки-погрешности)
    - [Нормы](#нормы)
    - [Обусловленность матрицы](#обусловленность-матрицы)
  - [Прямые методы решения СЛАУ](#прямые-методы-решения-слау)
- [Лекция 3](#лекция-3)
  - [Продолжаем про линейные методы](#продолжаем-про-линейные-методы)
    - [Метод Гаусса и LU-разложение](#метод-гаусса-и-lu-разложение)
    - [Уточнение решения](#уточнение-решения)
    - [Ещё немного об LU-разложении](#ещё-немного-об-lu-разложении)
    - [Метод квадратного корня (Холецкого)](#метод-квадратного-корня-холецкого)
  - [Итерационные методы решения СЛАУ](#итерационные-методы-решения-слау)
    - [Метод простой итерации](#метод-простой-итерации)
      - [Критерий сходимости МПИ](#критерий-сходимости-мпи)
    - [Метод Якоби](#метод-якоби)


# Инфо
Лектор - Васкевич <3

Семинарист - Левыкин Александр Иванович

# Лекция 1
Классификация вычислительных алгоритмов:
- Матричное вычисление (вычислительные методы линала)
- Задачи оптимизации, решение нелинейных уравнений и их систем
- Интерполирование и численное дифференцирование
- Численное приближение не-полиномов
- Численное интегрирование
- Численные и разностные методы для обыкновенных диффуров

**Численный метод** - числа и арифметические действия, рапсположенные в определённом порядке

## Корень числа
Простейший численный метод - нахождение положительного квадратного корня числа $a$. Испульзуем рекуррентную формулу:

$$
x_n = \frac{1}{2}(x_{n-1} + \frac{a}{x_{n-1}})
$$

Доказывается через выведение погрешности эпсилон (с условием $\frac{x_n}{\sqrt{a}} = 1 + \epsilon_n$), которая будет убывать быстрее геометрической прогрессии с шагом $1/2$, благодаря чему $\lim_{n \rarr \infty} x_n = \sqrt{a}$

*И... Реально работает! Вот код на Питоне:*
```python
a = 2
x = 1.0
for _ in range(1000):
    x = (x + a / x) / 2.0
print(x)
```

## Общие правила численного метода
1. Исходная непрерывная задача замещается дискретной задачей (см. пример выше, где мы избавились от непрерывной функции корня)
2. Добавляется количество шагов (параметр $N$, также называется **дискретным временем**)
3. Увеличивая дискретное время, мы приблизим результат численного метода к результату непрерывного сколь угодно близко (*на третий принцип порой забивают и даже очень часто - в зависимости от решаемой задачи*)

## Floating numbers
Описывается множеством $p, t, L, U$:
- p - основание системы счисления
- t - разрядность числа
- L - нижняя граница порядка
- U - верхняя граница порядка

Нормализованные числа (точка после первого значимого числа) с плавающей точкой обозначаются как множество $F_1$, оно обладает следующими свойствами:
- Числа в нём распределены неравномерно
- В частности, чем больше модуль числа, тем больше расстояние до соседей
- Между нулём и минальным числом имеется просвет, который в $k$ раз больше следующего числа

По стандарту IEEE 754 тип данных float имеет такое распределение:
- 1 бит - знак
- 8 бит - экспонента
- 23 бита - мантисса

*На double мы тут, видимо, забили... А, нет, слегка упомянули. ну в общем, это всё мы итак знаем*

Очень часто числа округляются. Есть несколько способов:
- Вниз - $R_d(x)$ - отбрасываем избыточную для формата часть мантиссы
- Вверх - $R_u(x)$ - стандартные правила округления
- Чётное - $R_e(x)$ - отличается от округления вверх только в случае, если $x$ находится ровно между двумя ЧПТ, тогда из них берётся то число, мантисса которого заканчивается на чётное число

## Погрешности
Для вещественного числа $a$ и его приближения $a^*$

Абсолютная погрешность - положительная величина $\Delta(a^*) \ge |a^* - a|$
- Значащую цифру в приближении числа называют **верной**, если $\Delta(a^*)$ не превосходит половины единицы этого разряда (читай как $0.5 * 10^{-t}$)

Относительная погрешность: $\delta(a^*) \ge \frac{|a^* - a|}{|a^*|}$

## Неустойчивость
Вычислительный алгоритм называется **неустойчивым**, если в ходе него происходит одно из событий:
- Потеря значащих цифр
- Переполнение ЧПТ

**Обусловленность** - чувствительность задачи к начальным условиям. Если погрешность результата задачи существенно больше погрешности входных данных, то задача будет **плохо обусловленной**

# Семинар 1
## Векторные и матричные нормы
Нормы $||.||_a, ||.||_b$ эквивалентны, если
$$
\forall X : \exist C_1, C_2 : C_1||X||_b \le ||X||_a \le C_2||X||_b
$$

# Лекция 2
## Численное решение СЛАУ
Разумеется, СЛАУ можно решить, в случае невырожденной матрицы (а иначе она и не решится) методом Крамера, однако считать определитель матрицы - задача не самая лёгкая

При этом в целом на компе при $n \le 10^6$ определитель считается за разумное время, однако есть и более эффективные методы: прямой и итерационный

Однако для этих численных методов возникает погрешность. Особенно высока она будет в плохо обусловленных системах. Таким образом становится актуальным оценка обусловленности матрицы

## Оценки погрешности
### Нормы
Все нормы линейны и удовлетворяют неравенству треугольника

Стандартные нормы:
- $||\vec{u}||_{\infty} = \max_{1 \le i \le n} |u_i|$ - кубическая
- $||\vec{u}||_1 = \sum_{i=1}^n |u_i|$ - октаэдрическая
- $||\vec{u}||_2 = \sqrt{\sum_{i=1}^n |u_i|^2}$ - евклидова

Пространство $\R^n$ снабжённое одной из норм будем обозначать как $L^n$

Линейное преобразование:
$$
\vec{v} = A\vec{u}
$$

Теперь определим норму для матрицы $A$:
$$
\forall ||.|| : ||A|| = \sup_{\vec{u} \ne 0} \frac{||A\vec{u}||}{||\vec{u}||}
$$
Эта норма также удовлетворяет:
- неравенству треугольника
- однородна ($||\lambda A|| = |\lambda|||A||$)
- неравенству треугольника 2.0 ($||AB|| \le ||A||||B||$) (а также $||A\vec{u}|| \le ||A|||\vec{u}|$)

Таким образом, из стандартных норм векторов получаем такие нормы для матриц:
- $||A||_{\infty} = \max_{1 \le i \le n} \sum_{j=1}^n |a_{ij}|$ - ищем максимальную по сумме строку матрицы
- $||A||_1 = \max_{1 \le j \le n} \sum_{i=1}^n |a_{ij}|$ - почти то же, что и первое, но теперь ищем максимальную сумму по столбцам
- $||A||_2 = \sqrt{\max_{1 \le i \le n} \lambda_i(A^*A)}$
  - $\lambda_i(A^*A)$ - это с.з. матрицы $A^*A$, при этом $A^* = A^T$
  - Если $A$ симметричная, тогда $\lambda_i(A^*A) = \lambda_i(A^2) = |\lambda_i(A)|^2$

*Напоминалка:* скалярное произведение $(\vec{u}, \vec{v}) = \sum_{i=1}^n u_iv_i$

Из определения нормы матрицы следует, что если норма матрицы согласована с нормой векторов, то для единичной матрицы норма будет 1

### Обусловленность матрицы
**О.** $\mu(A) = ||A||||A^{-1}||$ называется **числом обусловленности** матрицы. Несложно заметить, что выбор нормы влияет на числа обусловленности. Также обозначается как $cond(A)$

**Т.** Если возмущение матрицы $\Delta A$ таково, что:
$$
\mu(A) \frac{||\Delta A||}{||A||} < 1
$$
Тогда возмущение вектора-решения $\Delta \vec{u}$ удовлетворяет оценке:
$$
\frac{\Delta \vec{u}}{\vec{u}} \le \frac{\mu(A)}{1 - \mu(A) \frac{||\Delta A||}{||A||}}(\frac{||\Delta \vec{f}||}{||\vec{f}||} + \frac{||\Delta A||}{||A||})
$$
*Доказывается через преобразования последней оценки при помощи $A\vec{u} = \vec{f}$ и неравенства треугольника*

**С. 1** При $\Delta A \approx 0$, получаем оценку погрешности только правой части:
$$
\frac{\Delta \vec{u}}{\vec{u}} \le \mu(A)\frac{||\Delta \vec{f}||}{||\vec{f}||}
$$

**С. 2** Если $\Delta A \Delta \vec{u} \approx 0$, то имеет место такая оценка:
$$
\frac{||\Delta \vec{u}||}{||\vec{u}||} \le \frac{||\Delta \vec{f}||}{||\vec{f}||} + \frac{||\Delta A||}{||A||}
$$

В силу неравенства треугольника 2.0 и $||AA^{-1}|| = ||E|| = 1$ получаем, что $\mu(A) \ge 1$

Если число обусловленности не превышает 10, мы говорим о слабом влиянии ошибок входных данных. При $\mu >> 100$ система плохо обусловлена

*См. пример во второй лекции на стр. 29*

**Замечание 1** - определитель матрицы может быть большим, а число обусловленности - малым (пример - диагональная матрица с $\epsilon > 0$: $\det D = \epsilon^n, \mu(D) = 1$) и наоборот (верхнетреугольная матрица с $1$ на диагонали и $-1$ над диагональную имеет $\det A = 1$, но $\mu(A) = n2^{n-1}$)

**Замечание 2** - для невырожденной матрицы $A$ и любой нормы будет иметь место оценка снизу:
$$
\mu(A) \ge \frac{|\lambda_{\max}(A)|}{|\lambda_{\min}(A)|}
$$

## Прямые методы решения СЛАУ
Для диагональной невырожденной матрицы СЛАУ распадается на $n$ простейших уравнений и решается за $n$ делений

Для диагональной матрицы идём от конца к началу, вычисляя один компонент за другим. Число операций - $O(n^2)$ (**обратный ход метода Гаусса**)

Для неструктурированной матрицы сначала делаем приведение к треугольному виду, а затем как раньше. Называется алгоритм - **метод исключения Гаусса** (*реализовывали на первому курсе на императивке, так что описывать его считаю занятием не самым интересным (при этом там мы даже не только квадратные рассматривали)*)
- Прямой ход (превращение матрицы в верхнетреугольную) занимает $\approx \frac{3}{2}n^3$ операций

# Лекция 3
## Продолжаем про линейные методы
### Метод Гаусса и LU-разложение
**LU-разложение матрицы A** - разложение, получаемое в ходе прямого хода метода Гаусса, где:
- A - исходная матрица
- U - итоговая верхнетреугольная матрица
- L - нижнетреугольная матрица коэффициентов, используемых для приведения матрицы A к верхнетреугольному виду (по факту представима через произведение $n-1$ отдельных матриц-столбцов)
  - $k < j : l_{kj} = 0$
  - $l_{jj} = 1$
  - $k > j : l_{kj} = \frac{a_{kj}^{(j-1)}}{a_{jj}^{(j-1)}}$

На практике в прямом ходе метода Гаусса на каждом шаге делается выбор строки с макимальным по модулю элементом для данного столбца. Потом эта строка меняется местами с текущей строкой, а столбец ниже зануляется

Проблем с методом не будет, если у матрицы есть **диагональное преобладание:** 
$$\forall i : |a_{ii}| \ge \sum_{j=1, j \ne i}^N |a_{ij}|$$

### Уточнение решения
- Находим **вектор невязки** $\vec{r}_1 = \vec{f} - A\vec{u}_1$
- Далее решаем систему $A\vec{\epsilon}_1 = \vec{r}_1$
- Уточнённое решение $\vec{u}_2 = \vec{u}_1 + \vec{\epsilon}_1$
- Можно продолжать итеративно

---

### Ещё немного об LU-разложении
Зная LU разложение, можно провести такие модфикации:
$$
A\vec{u} = \vec{f} \\
LU\vec{u} = \vec{f} \\
U\vec{u} = \vec{v} \rArr L\vec{v} = \vec{f}
$$
Решение СЛАУ $L\vec{v} = \vec{f}$ с нижнетреугольной матрицей может быть в некоторых случаях найдено проще, чем по классическому алгоритму Гаусса, *однако... по сути мы не получаем в таком случае никакой экономии вычислительных мощностей, а будто бы даже наоборот*

Разложение на верхне и нижнетреугольную матрицу достаточно просто делается из уравнения $A = LU$. По следующим формулам:
- $u_{ij} = a_{ij} - \sum_{k=1}^{i-1} l_{ik}u_{kj}$ (от элемента A в строке отнимаем произведение в столбце матрицы U над ним на часть строки такой же длины в матрице L)
- $l_{ij} = \frac{a_{ij} - \sum_{k=1}^{j-1} l_{ik}u_{kj}}{d_{jj}}$ (от элемента из A в столбце отнимаем произведение строки из матрицы L слева от него на столбец над ним из матрицы U такой же длины. Результат делим диагональный элемент из U)
- Сначала пишем первую строку матрицы U (идентична строке матрицы A)
- Затем вычисляем столбец матрицы L
- И далее поочерёдно считаем строку и столбец U и L соответственно

### Метод квадратного корня (Холецкого)
Если матрица **симметричная и положительно определённая**, то найдётся такое LU-разложение, что $LL^T = A$, причём:
- $l_{ii} = \sqrt{a_{ii} - \sum_{k=1}^{j-1} l_{ik}^2}$
- $l_{ij} = \frac{a_{ij} - \sum_{k=1}^{j-1} l_{ik}l_{jk}}{l_{ii}}$

Опасность тут представляет то, что под корнем в результате ошибок округления могут возникнуть отрцательные числа, хотя в по условию быть такого не должно

Далее решаем СЛАУ через LU-разложение по классике

## Итерационные методы решения СЛАУ
### Метод простой итерации
Умножим части СЛАУ $A\vec{u} = \vec{f}$ на некий скаляр $t$ и затем прибавим к обеим частям $\vec{u}$ и проведём преобразования:
$$
\vec{u} + tA\vec{u} = \vec{u} + t\vec{f} \\
\vec{u} = (E - tA)\vec{u} + t\vec{f} \\
B = (E - tA), \vec{F} = t\vec{f} \\ 
\vec{u} = B\vec{u} + \vec{F}
$$
Получили главный оператор итерационного метода:
$$
\vec{u}_{k+1} = B\vec{u}_k + \vec{F}
$$
Работает он так:
- Задаём начальный вектор $\vec{u}_0$ (можно даже нулевой)
- Считаем невязку $\vec{r}_0 = \vec{f} - A\vec{u}_0$
- Корректируем решение $\vec{u}_1 = \vec{u}_0 + t\vec{r}_0$
- Понимаем, что отбросив невязку, мы получаем в точности $\vec{u}_{k+1} = B\vec{u}_k + \vec{F}$
- Запускаем реешние на следующие итерации и идём пить матча-латте

#### Критерий сходимости МПИ
**Т.** Если $||B|| = q < 1$, то МПИ сходится к решению СЛАУ со скоростью геометрической прогрессии с шагом $q^{-1}$

*Доказывается через формулу МПИ и неравенство треугольника 2.0*

Для сходимости с точностью $\epsilon$ нужно шагов:
$$
k \ge \frac{1}{\ln q} \ln \frac{\epsilon}{||\vec{u} - \vec{u}_0||}
$$

**Т.** Пусть СЛАУ имеет единственное решение, тогда МПИ будет сходиться тогда и только тогда, когда все с.з. матрицы B по абсолютным значениям меньше единицы

*Путём не самых красивых преобразований делается оценка погрешности решения МПИ* $\le \frac{1 - q^k}{1 - q}d \le \frac{d}{1 - q}$, где $d$ - максимальная ошибка за все итерации. Из формулы следует, что итоговая погрешность не зависит от количества итераций

### Метод Якоби
- Разобьём матрицу на нижнетреугольную без диагонали, диагональную и верхнетреугольную без диагонали $A = L + D + U$
- Подставим разбиение в СЛАУ: $L\vec{u} + D\vec{u} + U\vec{u} = \vec{f}$
- Раставляем индексы у векторов $u$: $L\vec{u}_k + D\vec{u}_{k+1} + U\vec{u}_k = \vec{f}$
- Предполагая, что $D$ не вырождена, задаём $\vec{u}_0$ и получаем итерационный процесс
- $\vec{u}_{k+1} = -D^{-1}(L + U)\vec{u}_k + D^{-1}\vec{f}$
  - Можно записать как $\vec{u}_{k-1} = B\vec{u}_k + \vec{F}$ с заменами
  - $\vec{F} = D^{-1}\vec{f}$
  - $B = -D^{-1}(L + U)$ - матрица с нулями на диагонали и остальными элементами считающимися как $b_{ij} = -\frac{a_{ij}}{a_{ii}}$

**Т о достаточном условии сходимости метода Якоби.** Если матрица $A$ имеет строгое диагональное преобладание, тогда её решение методом Якоби сходится к решению этой СЛАУ (*Доказывается через определение матрицы $B$ из которого следует, что при условии теоремы $||B||_{\infty} = q < 1$*)

**Т. о критерии сходимости метода Якоби.** Метод Якоби сходится тогда и только тогда, когда корни $\lambda_{ii}$ уравнения $\det A' = 0$, где $a'_{ii} = \lambda_{ii}, a'_{ij} = a_{ij}$ по модулю не превосходят единицу (*доказывается через тот факт, что все $\lambda_{ii}$ из этого уравнения - это с.з. матрицы $B$, за счёт чего мы переходим к критерию сходимости МПИ*)