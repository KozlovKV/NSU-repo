# Инфо
Лектор - Неделько Виктор Михайлович

Семинарист - Денис Михайлович Михайлапов ([Телега](https://t.me/d_mihailapov))

[Хэндбук от Яндекса](https://education.yandex.ru/handbook/ml) (*рекомендует не читать его сразу, а использовать как справочник*)

Другие книжки:
![](./lec/24-09-09%20-%20books.JPG)

[Kaggle](kaggle.com) - это мега-круто! Проводятся денежные конкурсы, публикуются решения задач, на рейтинг с Каггла работодатели обращают много внимания

# Список экзаменационных вопросов (*неточный*)
1. 

# Лекция 1
Речь будет идти о "классическом" машинном обучении, то есть о методах, работающих с таблицами данных (изображения, речь, текст не являются табличными объектами и с ними работают нейронки), то есть набор чисел, которые связаны между собой

## Классификация
Есть табличные данные с несколькими характеристиками, одна из которых - целевая. Необходимо будет используя остальные признаки, определить значение целевого

### Метод прецедентов
Общая идея - смотрим на уже имеющиеся точки, чтобы сделать выводы о новой

#### Метод `k`-соседей
Смотрит на `k` ближайших значений по заданным параметрам и принимаем ключевое значение то же, что и бОльшего количества соседей
- Выбор `k` по большей части интуитивен, но в целом, зависит от выборки (очень желательно, чтобы `k` было намного меньше выборки)
- Второй вариант выбора `k` - это кросс-валидация - выбираем разные `k` и проверяем их корректность (надо делить выборку на тренировочную и тестовую часть)

#### Парзеновское окно
Смотрит на область, в которой размещаются точки с одинаковым ключевым признаком. Если новая точка располагается в нескольких областях, смотрим на то, каких точек больше

#### Ядровое сглаживание (kernel-based)
Вклад объектов в прогноз будет оцениваться при помощи фильтрующей функции расстояния (хороший вариант - гауссова функция расстояния)

Вот пример с использованием ядра Коши:
$$
f(x) = \frac{1}{w}\sum^N_{i=1}y_iw_i \\
w = \sum^N_{i=1} w_i \\
w_i = \phi(\rho(x, x_i)) \\
\phi(z) = \frac{1}{1 + (\frac{z}{r})^2}
$$

#### Заключение
Метод прецедентов считается самым простым, но и потому работает хорошо не всегда. Достаточно не плох он будет при малом количестве переменных, а вот при большом количестве переменных будет работать сильно хуже (будут возникать шумовые переменные, по которым разделение на классы будет затруднено)

Таким образом, метод `k`-соседей будет актуален в двух случаях:
- Мало характеристических переменных либо мы смогли отфильтровать нужные
- Как дополнение к какому-то комплексному методу или нейронкам (так, у текста все переменные после прохода через энкодер будут полезными)


# Лекция 2
## (Квази-)линейные методы классификации
"Квази" заключается в том, что мы решение нелинейной задачи сводим к линейной

### Линейный/квадратичный дискриминант
Метод, основанный на матстате.

Мы предполагаем, что распределение выборки имеет нормальную форму. В таком случае нам надо найти параметры выборки (*рассматриваем 2 класса: $y \in \{-1, 1\}$*)

*Напомним функцию нормального распределния:
$$
\phi_y(x) = \frac{1}{\sqrt{2\pi}\sigma_y}e^{-\frac{(x - \mu_y)^2}{2\sigma_y^2}}
$$

Далее нам остаётся наложить друг на друга плотности распределения для каждого класса. У какого класса в заданной точке выше вероятность, тот и более вероятен. Область пересечения нескольких классов будет считаться ошибкой (то есть ошибку можно посчитать, взяв интеграл по этим областям)

Сама решающая функция Байеса будет выглядеть так:
$$
f(x) = \arg \max_y \phi(x, y) \\
\phi(x, y) = \phi_y(x) P(y)
$$
- $\phi_y(x)$ - функция распределния класса $y$ (нормального распределения)

Если вероятность ошибки достаточно высока, то разумнее всего будет выдать пользователю информацию о том, какова вероятность отнесения к различным классам

Можно также записать решающую функцию через логарифм:
$$
f(x) = \begin{cases}
  1, l(x) \ge 0 \\
  -1, l(x) < 0
\end{cases} \\
l(x) = \ln \phi_{1}(x) - \ln \phi_{-1}(x) + \ln \frac{P(1)}{P(-1)}
$$
Подставив сюда функции нормально распределния с параметрами $\sigma, \mu$, получим:
$$
l(x) = \frac{(x - \mu_{-1})^2}{\sigma_{-1}^2} - \frac{(x - \mu_{1})^2}{\sigma_1^2} + \ln \frac{\sigma_{-1}}{\sigma_1} + \ln \frac{P(1)}{P(-1)}
$$
При $l(x) = 0$ это будет квадратное уравнение, имеющее 2 корня, либо 1 при $\sigma_1 = \sigma_{-1}$ либо 0 корней в вырожденных случаях

#### Случай N переменных
*Также рассматриваем случай двух классов*
Для каждого класса $y$:
- $\mu_y$ - вектор матожиданий
- $\lambda_y$ - ковариационная матрица

Решающая функция та же: $l(x) = \ln \phi_{1}(x) - \ln \phi_{-1}(x) + \ln \frac{P(1)}{P(-1)}$, а вот функция плотности будет такой:
$$
\phi_y(x) = \frac{1}{\sqrt{2\pi}|\lambda_y|^{n/2}}e^{-\frac{1}{2}Q_y(x)} \\
Q_y(x) = (x - \mu_y)^T(\\lambda_y)^{-1}(x - \mu_y)
$$
Подставляем формулу плотности в $l(x)$ и получаем:
$$
2l(x) = Q_{-1}(x) - Q_1(x) + \ln |\lambda_{-1}| - \ln |\lambda_1| + 2\ln P(1) - 2\ln P(-1) 
$$
Подставляем значения $Q_y(x)$ и после преобразований получаем:
$$
2l(x) = x^T A x + bx + c \\ 
A = (\lambda_{-1})^{-1} - (\lambda_1)^{-1} \\
b = 2\mu_1(\lambda_{-1})^{-1} - 2\mu_{-1}(\lambda_1)^{-1} \\
c = \mu_{-1}^T(\\lambda_{-1})^{-1}\mu_{-1} - \mu_1^T(\\lambda_1)^{-1}\mu_1 + \ln |\lambda_{-1}| - \ln |\lambda_1| + 2\ln P(1) - 2\ln P(-1) 
$$

Уравнение $l(x) = 0$ в общем случае даст нам плоскость второго порядка, которая при равенстве ковариационных матриц выродится в гиперплоскость

#### Примерные параметры
- $N_y$ - число объектов класса $y$
- $N$ - общее число объектов
- $I_y$ - множество индексов объектов класса $y$

Тогда имеем:
$$
\approx{P(y)} = \frac{N^y}{N} \\ 
\approx{\mu_y} = \frac{1}{N_y}\sum_{i \in I_y} x_i \\
\approx{\lambda_y} = \frac{1}{N_y} \sum_{i \in I_y} (x_i - \approx{\mu_y})(x_i - \approx{\mu_y})^T
$$

---

Для линейного и квадратичного правила нужно разное количество объектов в выборке: для линейного правила от 1000 объектов с 100 параметрами мы получим адекватный результат, тогда как для квадратичного правила этого уже не хватит

#### Дискриминант Фишера
Линейный дискриминант Фишера используется в том случае, когда мы хотим понять, а хороша ли граница, которую мы провели между классами

*А дальше вообще ни хера не понял, но ладно...*

#### Логистическая регрессия
*А тут вообще ни-чер-та не понятно*

#### Наивный байесовский метод

#### Машина опорных векторов
