# Инфо
Лектор - Неделько Виктор Михайлович

Семинарист - Денис Михайлович Михайлапов ([Телега](https://t.me/d_mihailapov))

[Хэндбук от Яндекса](https://education.yandex.ru/handbook/ml) (*рекомендует не читать его сразу, а использовать как справочник*)

Другие книжки:
![](./lec/24-09-09%20-%20books.JPG)

[Kaggle](kaggle.com) - это мега-круто! Проводятся денежные конкурсы, публикуются решения задач, на рейтинг с Каггла работодатели обращают много внимания

# 24.09.09 - Лекция
Речь будет идти о "классическом" машинном обучении, то есть о методах, работающих с таблицами данных (изображения, речь, текст не являются табличными объектами и с ними работают нейронки), то есть набор чисел, которые связаны между собой

## Классификация
Есть табличные данные с несколькими характеристиками, одна из которых - целевая. Необходимо будет используя остальные признаки, определить значение целевого

### Метод прецедентова
Общая идея - смотрим на уже имеющиеся точки, чтобы сделать выводы о новой

#### Метод `k`-соседей
Смотрит на `k` ближайших значений по заданным параметрам и принимаем ключевое значение то же, что и бОльшего количества соседей
- Выбор `k` по большей части интуитивен, но в целом, зависит от выборки (очень желательно, чтобы `k` было намного меньше выборки)
- Второй вариант выбора `k` - это кросс-валидация - выбираем разные `k` и проверяем их корректность (надо делить выборку на тренировочную и тестовую часть)

#### Парзеновское окно
Смотрит на область, в которой размещаются точки с одинаковым ключевым признаком. Если новая точка располагается в нескольких областях, смотрим на то, каких точек больше
- Удачной модификацией будет добавление фильтрующей функции (хороший вариант - гауссова функция расстояния)

#### Заключение
Метод прецедентов считается самым простым, но и потому работает хорошо не всегда. Достаточно не плох он будет при малом количестве переменных, а вот при большом количестве переменных будет работать сильно хуже (будут возникать шумовые переменные, по которым разделение на классы будет затруднено)

Таким образом, метод `k`-соседей будет актуален в двух случаях:
- Мало характеристических переменных либо мы смогли отфильтровать нужные
- Как дополнение к какому-то комплексному методу или нейронкам (так, у текста все переменные после прохода через энкодер будут полезными)

### (Квази-)линейные методы классификации
"Квази" заключается в том, что мы решение нелинейной задачи сводим к линейной

#### Линейный дискриминант
Метод, основанный на матстате.

#### Квадратичный дискриминант

#### Дискриминант Фишера

#### Логистическая регрессия

#### Наивный байесовский метод

#### Машина опорных векторов
